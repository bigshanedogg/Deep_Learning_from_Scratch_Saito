{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mnist\n",
    "import pickle\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load MNIST dataset\n",
    "- 책에서 나온 라이브러리는 사용이 안되는 것 같아 다른 라이브러리를 사용 (다행히 MNIST를 제공하는 라이브러리는 많음)\n",
    "- chapter 4, 5 모두 mnist를 사용하는데 매번 라이브러리에서 불러오기 귀찮으니 pickle로 저장해둔다.\n",
    "- train/test, image/label로 나눈다. \n",
    "- 현재 shape는 (size, m, n)으로 m*n matrix가 size개만큼 있는 상태. 우선 책과 shape를 맞추기 위해 m X n matrix를 길이가 m*n인 vector로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_images = mnist.train_images()\n",
    "train_labels = mnist.train_labels()\n",
    "test_images = mnist.test_images()\n",
    "test_labels = mnist.test_labels()\n",
    "temp = [train_images, train_labels, test_images, test_labels] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#mnist 데이터 pickle로 저장\n",
    "with open(\"mnist_dataset.txt\", mode=\"wb\") as fp : \n",
    "    dataset = [train_images, train_labels, test_images, test_labels] \n",
    "    pickle.dump(dataset, fp)\n",
    "    \n",
    "#mnist 데이터 pickle로 로드\n",
    "temp = []\n",
    "with open(\"mnist_dataset.txt\", mode=\"rb\") as fp : \n",
    "    temp = pickle.load(fp)\n",
    "train_images, train_labels, test_images, test_labels = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n"
     ]
    }
   ],
   "source": [
    "print(train_images.shape)\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#reshape (28, 28) -> (784)\n",
    "temp[0] = np.reshape(temp[0], [temp[0].shape[0], -1])\n",
    "temp[2] = np.reshape(temp[2], [temp[2].shape[0], -1])\n",
    "x_train, t_train, x_test, t_test = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#shape check\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   3  18  18  18 126 136 175  26 166 255\n",
      " 247 127   0   0   0   0   0   0   0   0   0   0   0   0  30  36  94 154\n",
      " 170 253 253 253 253 253 225 172 253 242 195  64   0   0   0   0   0   0\n",
      "   0   0   0   0   0  49 238 253 253 253 253 253 253 253 253 251  93  82\n",
      "  82  56  39   0   0   0   0   0   0   0   0   0   0   0   0  18 219 253\n",
      " 253 253 253 253 198 182 247 241   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0  80 156 107 253 253 205  11   0  43 154\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0  14   1 154 253  90   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0 139 253 190   2   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0  11 190 253  70   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  35 241\n",
      " 225 160 108   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0  81 240 253 253 119  25   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0  45 186 253 253 150  27   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0  16  93 252 253 187\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0 249 253 249  64   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0  46 130 183 253\n",
      " 253 207   2   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0  39 148 229 253 253 253 250 182   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0  24 114 221 253 253 253\n",
      " 253 201  78   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0  23  66 213 253 253 253 253 198  81   2   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0  18 171 219 253 253 253 253 195\n",
      "  80   9   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "  55 172 226 253 253 253 253 244 133  11   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0 136 253 253 253 212 135 132  16\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "#checking data\n",
    "img = x_train[0]\n",
    "label = x_train[0]\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784,)\n",
      "(28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(img.shape)\n",
    "img = img.reshape(28, 28)\n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function showing label as image\n",
    "def img_show(img) : \n",
    "    pil_img = Image.fromarray(np.uint8(img))\n",
    "    pil_img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#show data[0] label as image\n",
    "img_show(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<br><hr><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load function \n",
    "- 앞서 작성한 loss & activation function을 가져온다. (나중에 .py로 옮겨서 라이브러리처럼 사용할 것)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sigmoid_function : 주어진 값을 0~1 사이의 값으로 변환해주는 S자 모양의 함수. 0보다 크면 0.5보다 크게, 0보다 작으면 0.5보다 작게 변환된다.\n",
    "def sigmoid_function(x) : \n",
    "    return (1/(1+np.exp(-x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#identity_function : 마지막 출력 함수에서 주로 사용하는 활성화 함수. 형식을 맞추기 위해 사용될 뿐 실제 의미는 없다.\n",
    "def identity_function(x) :\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#softmax_function\n",
    "def softmax_function(x) : \n",
    "    max_by_row = np.amax(x, axis=1)\n",
    "    max_by_row = np.reshape(max_by_row, [max_by_row.shape[0],1])\n",
    "    nom = np.exp(x-max_by_row)\n",
    "    denom = np.sum(np.exp(x-max_by_row), axis=1)\n",
    "    denom = np.reshape(denom, [denom.shape[0],1])\n",
    "    return nom/denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#CEE (cross_entropy_error, 교차엔트로피오차)\n",
    "def CEE(y_p, y_t) : #y_p: predicted vector, y_t: actual_vector\n",
    "    if y_t.ndim == 1 : \n",
    "        y_t = y_t.reshape(1, y_t.size)\n",
    "        y_p = y_p.reshape(1, y_p.size)\n",
    "        \n",
    "    delta = 1e-7\n",
    "    return -1/y_p.shape[0]*np.sum(y_t*np.log(y_p + delta)) #y_p가 0이 되어 log 계산시 -inf가 나오는 현상 방지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ㅜnumerical differentian function (수치 미분 함수)\n",
    "def numerical_gradient(f, x) : #편미분은 특정 변수에 대한 미분을 의미했는데, \n",
    "    #gradient는 각각의 변수에 대해 전부 편미분하여 그 값을 numpy array형태로 반환한다. 실제 내부 코드는 partial_diff와 크게 다르지 않다.\n",
    "    h = 1e-4 #0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    for idx in range(0, x.size) : \n",
    "        temp = x[idx]\n",
    "        \n",
    "        x[idx] = temp + h\n",
    "        fxh1 = f(x)\n",
    "        x[idx] = temp - h \n",
    "        fxh2 = f(x)\n",
    "        \n",
    "        grad[idx] = (fxh1-fxh2)/(2*h)\n",
    "        x[idx] = temp\n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><hr><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3. Modeling MLP\n",
    "- tensorflow와 마찬가지로, numpy로 구현하는 MLP는 간선에 들어갈 가중치와 노드들만 정의해두고 간선의 연결(굳이 이름 붙이자면, 노드와 노드 간의 연산)을 수식(이 경우, \"matmul(X*W)+b\")을 통해 정의한다.\n",
    "- init_network()는 간선의 가중치 값을 담는 사전인데, 현재는 hidden_layer의 개수와 layer 내의 node수는 수동으로 정해줘야 한다. (이후 용도에 따라 parameter로 받도록 수정 가능할 듯)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_network() : #간선의 가중치 값을 담는 사전\n",
    "    network = {} \n",
    "    network[\"W1\"] = np.random.random_sample((784,50))\n",
    "    network[\"b1\"] = np.random.random_sample((50,))\n",
    "    network[\"W2\"] = np.random.random_sample((50,100))\n",
    "    network[\"b2\"] = np.random.random_sample((100,))\n",
    "    network[\"W3\"] = np.random.random_sample((100,10))\n",
    "    network[\"b3\"] = np.random.random_sample((10,))\n",
    "    \n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(network, x) : \n",
    "    W1, W2, W3 = network[\"W1\"], network[\"W2\"], network[\"W3\"]\n",
    "    b1, b2, b3 = network[\"b1\"], network[\"b2\"], network[\"b3\"]\n",
    "    \n",
    "    a1 = np.dot(x, W1) + b1\n",
    "    z1 = sigmoid_function(a1)\n",
    "    a2 = np.dot(z1, W2) + b2\n",
    "    z2 = sigmoid_function(a2) \n",
    "    a3 = np.dot(z2, W3) + b3\n",
    "    y = identity_function(a3)\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "network = init_network() #네트워크 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x, t = x_test, t_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0892\n"
     ]
    }
   ],
   "source": [
    "accuracy_cnt = 0 \n",
    "batch_size = 32\n",
    "for i in range(0, len(x), batch_size) : #mini_batch 처리\n",
    "    if i+batch_size >= len(x) :\n",
    "        x_batch = x[i:len(x)] #마지막 부분은 예외처리 (list index range)\n",
    "    else : \n",
    "        x_batch = x[i:i+batch_size] \n",
    "        \n",
    "    y_batch = predict(network, x_batch)\n",
    "    p = np.argmax(y_batch, axis=1)\n",
    "    t_batch = np.reshape(t[i:i+batch_size], [-1,len(t[i:i+batch_size])]) #label이 matrix이므로 flatten하여 vector로 변환\n",
    "    accuracy_cnt += np.sum(p==t_batch)\n",
    "\n",
    "print(\"Accuracy:\", str(float(accuracy_cnt)/len(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "892\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MLP : \n",
    "    def __init__(self, input_size, hidden_size_list, output_size, weight_init_std = 0.01) : \n",
    "        #가중치 초기화\n",
    "        self.params = {}\n",
    "        self.hidden_len = len(hidden_size_list)\n",
    "        for i in range(1, self.hidden_len+2) : #hidden_layer의 수 + output_layer\n",
    "            if i==self.hidden_len+1 : \n",
    "                self.params[\"W\"+str(i)] = weight_init_std * np.random.randn(hidden_size_list[i-2], output_size) \n",
    "                self.params[\"b\"+str(i)] = np.zeros(output_size)\n",
    "            elif i==1 : \n",
    "                self.params[\"W\"+str(i)] = weight_init_std * np.random.randn(input_size, hidden_size_list[i-1]) \n",
    "                self.params[\"b\"+str(i)] = np.zeros(hidden_size_list[i-1])\n",
    "            else :     \n",
    "                self.params[\"W\"+str(i)] = weight_init_std * np.random.randn(hidden_size_list[i-2], hidden_size_list[i-1]) \n",
    "                self.params[\"b\"+str(i)] = np.zeros(hidden_size_list[i-1])\n",
    "            \n",
    "    \n",
    "    def predict(self, x) : #x: 입력 데이터\n",
    "        a = 0\n",
    "        z = x\n",
    "        \n",
    "        i = 1\n",
    "        for i in range(1, self.hidden_len+1) : \n",
    "            a = np.dot(z, self.params[\"W\"+str(i)]) + self.params[\"b\"+str(i)]\n",
    "            z = sigmoid_function(a)\n",
    "        i+=1\n",
    "        a = np.dot(z, self.params[\"W\"+str(i)]) + self.params[\"b\"+str(i)]\n",
    "        y = softmax_function(a)\n",
    "    \n",
    "        return y\n",
    "    \n",
    "    \n",
    "    def loss(self, x, t) : #x: 입력 데이터, t: 정답 레이블\n",
    "        y = self.predict(x)\n",
    "        return CEE(y, t)\n",
    "    \n",
    "    \n",
    "    def accuracy(self, x, t) : #x: 입력 데이터, t: 정답 레이블\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y==t)/float(x.shape[0])\n",
    "        return accuracy\n",
    "    \n",
    "    \n",
    "    #ㅜnumerical differentian function for weigths (수치 미분 함수)\n",
    "    def weight_numerical_gradient(self, f, x) : #편미분은 특정 변수에 대한 미분을 의미했는데, \n",
    "        #gradient는 각각의 변수에 대해 전부 편미분하여 그 값을 numpy array형태로 반환한다. 실제 내부 코드는 partial_diff와 크게 다르지 않다.\n",
    "        h = 1e-4 #0.0001\n",
    "        grad = np.zeros_like(x)\n",
    "\n",
    "        for i in range(0, x.shape[0]) : \n",
    "            for j in range(0, x.shape[1]) : \n",
    "                temp = x[i][j]\n",
    "\n",
    "                x[i][j] = temp + h\n",
    "                fxh1 = f(x[i][j])\n",
    "                x[i][j] = temp - h \n",
    "                fxh2 = f(x[i][j])\n",
    "\n",
    "                grad[i][j] = (fxh1-fxh2)/(2*h)\n",
    "                x[i][j] = temp\n",
    "\n",
    "        return grad\n",
    "    \n",
    "    \n",
    "    #ㅜnumerical differentian function for bias (수치 미분 함수)\n",
    "    def bias_numerical_gradient(self, f, x) : #편미분은 특정 변수에 대한 미분을 의미했는데, \n",
    "        #gradient는 각각의 변수에 대해 전부 편미분하여 그 값을 numpy array형태로 반환한다. 실제 내부 코드는 partial_diff와 크게 다르지 않다.\n",
    "        h = 1e-4 #0.0001\n",
    "        grad = np.zeros_like(x)\n",
    "\n",
    "        for i in range(0, x.shape[0]) : \n",
    "            temp = x[i]\n",
    "\n",
    "            x[i] = temp + h\n",
    "            fxh1 = f(x[i])\n",
    "            x[i] = temp - h \n",
    "            fxh2 = f(x[i])\n",
    "\n",
    "            grad[i] = (fxh1-fxh2)/(2*h)\n",
    "            x[i] = temp\n",
    "\n",
    "        return grad\n",
    "    \n",
    "    \n",
    "    #수치미분 함수를 이용해 각 가중치에 대응하는 gradient 사전을 얻음\n",
    "    def get_gradient(self, x, t) :\n",
    "        loss_value = self.loss(x, t)\n",
    "        loss_W = lambda W: loss_value\n",
    "\n",
    "        grads = {}\n",
    "        for i in range(1, self.hidden_len+2) : #hidden_layer의 수 + output_layer\n",
    "            grads[\"W\"+str(i)] = self.weight_numerical_gradient(loss_W, self.params[\"W\"+str(i)])\n",
    "            grads[\"b\"+str(i)] = self.bias_numerical_gradient(loss_W, self.params[\"b\"+str(i)])\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><hr><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.random.rand(100,784)\n",
    "t = np.random.rand(100,10)\n",
    "y = net.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#mnist 데이터 pickle로 로드\n",
    "temp = []\n",
    "\n",
    "with open(\"mnist_dataset.txt\", mode=\"rb\") as fp : \n",
    "    temp = pickle.load(fp)\n",
    "    temp[0] = np.reshape(temp[0], [temp[0].shape[0], -1])\n",
    "    temp[2] = np.reshape(temp[2], [temp[2].shape[0], -1])\n",
    "    \n",
    "    nb_classes = 10\n",
    "    temp[1] = np.array([temp[1]]).reshape(-1)\n",
    "    temp[1] = np.eye(nb_classes)[temp[1]]\n",
    "    temp[3] = np.array([temp[3]]).reshape(-1)\n",
    "    temp[3] = np.eye(nb_classes)[temp[3]]\n",
    "    \n",
    "x_train, y_train, x_test, y_test = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = MLP(input_size=x_train.shape[1], hidden_size_list=[50,100], output_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grad = net.get_gradient(x, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1_weights : (784, 50)\tW1_gradients : (784, 50)\n",
      "b1_weights : (50,)\tb1_gradients : (50,)\n",
      "b3_weights : (10,)\tb3_gradients : (10,)\n",
      "W3_weights : (100, 10)\tW3_gradients : (100, 10)\n",
      "b2_weights : (100,)\tb2_gradients : (100,)\n",
      "W2_weights : (50, 100)\tW2_gradients : (50, 100)\n"
     ]
    }
   ],
   "source": [
    "for x in net.params.keys() : \n",
    "    print(x,\"_weights : \",net.params[x].shape,\"\\t\",x,\"_gradients : \",grad[x].shape, sep=\"\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#setting hyper parameter\n",
    "iters_num = 1000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 2**7\n",
    "learning_rate = 0.1\n",
    "iter_per_epoch = int(max(train_size/batch_size, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "468"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iter_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#result\n",
    "train_loss_list = [] \n",
    "train_acc_list = []\n",
    "test_acc_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "epoch:     0]]] loss: 2.297773\ttrai acc: 0.112367, test acc: 0.113500\n",
      "epoch: 1\n",
      "epoch: 2\n",
      "epoch: 3\n",
      "epoch: 4\n",
      "epoch: 5\n",
      "epoch: 6\n",
      "epoch: 7\n",
      "epoch: 8\n",
      "epoch: 9\n",
      "epoch: 10\n",
      "epoch: 11\n",
      "epoch: 12\n",
      "epoch: 13\n",
      "epoch: 14\n",
      "epoch: 15\n",
      "epoch: 16\n",
      "epoch: 17\n",
      "epoch: 18\n",
      "epoch: 19\n",
      "epoch: 20\n",
      "epoch: 21\n",
      "epoch: 22\n",
      "epoch: 23\n",
      "epoch: 24\n",
      "epoch: 25\n",
      "epoch: 26\n",
      "epoch: 27\n",
      "epoch: 28\n",
      "epoch: 29\n",
      "epoch: 30\n",
      "epoch: 31\n",
      "epoch: 32\n",
      "epoch: 33\n",
      "epoch: 34\n",
      "epoch: 35\n",
      "epoch: 36\n",
      "epoch: 37\n",
      "epoch: 38\n",
      "epoch: 39\n",
      "epoch: 40\n",
      "epoch: 41\n",
      "epoch: 42\n",
      "epoch: 43\n",
      "epoch: 44\n",
      "epoch: 45\n",
      "epoch: 46\n",
      "epoch: 47\n",
      "epoch: 48\n",
      "epoch: 49\n",
      "epoch: 50\n",
      "epoch: 51\n",
      "epoch: 52\n",
      "epoch: 53\n",
      "epoch: 54\n",
      "epoch: 55\n",
      "epoch: 56\n",
      "epoch: 57\n",
      "epoch: 58\n",
      "epoch: 59\n",
      "epoch: 60\n",
      "epoch: 61\n",
      "epoch: 62\n",
      "epoch: 63\n",
      "epoch: 64\n",
      "epoch: 65\n",
      "epoch: 66\n",
      "epoch: 67\n",
      "epoch: 68\n",
      "epoch: 69\n",
      "epoch: 70\n",
      "epoch: 71\n",
      "epoch: 72\n",
      "epoch: 73\n",
      "epoch: 74\n",
      "epoch: 75\n",
      "epoch: 76\n",
      "epoch: 77\n",
      "epoch: 78\n",
      "epoch: 79\n",
      "epoch: 80\n",
      "epoch: 81\n",
      "epoch: 82\n",
      "epoch: 83\n",
      "epoch: 84\n",
      "epoch: 85\n",
      "epoch: 86\n",
      "epoch: 87\n",
      "epoch: 88\n",
      "epoch: 89\n",
      "epoch: 90\n",
      "epoch: 91\n",
      "epoch: 92\n",
      "epoch: 93\n",
      "epoch: 94\n",
      "epoch: 95\n",
      "epoch: 96\n",
      "epoch: 97\n",
      "epoch: 98\n",
      "epoch: 99\n",
      "epoch: 100\n",
      "epoch: 101\n",
      "epoch: 102\n",
      "epoch: 103\n",
      "epoch: 104\n",
      "epoch: 105\n",
      "epoch: 106\n",
      "epoch: 107\n",
      "epoch: 108\n",
      "epoch: 109\n",
      "epoch: 110\n",
      "epoch: 111\n",
      "epoch: 112\n",
      "epoch: 113\n",
      "epoch: 114\n",
      "epoch: 115\n",
      "epoch: 116\n",
      "epoch: 117\n",
      "epoch: 118\n",
      "epoch: 119\n",
      "epoch: 120\n",
      "epoch: 121\n",
      "epoch: 122\n",
      "epoch: 123\n",
      "epoch: 124\n",
      "epoch: 125\n",
      "epoch: 126\n",
      "epoch: 127\n",
      "epoch: 128\n",
      "epoch: 129\n",
      "epoch: 130\n",
      "epoch: 131\n",
      "epoch: 132\n",
      "epoch: 133\n",
      "epoch: 134\n",
      "epoch: 135\n",
      "epoch: 136\n",
      "epoch: 137\n",
      "epoch: 138\n",
      "epoch: 139\n",
      "epoch: 140\n",
      "epoch: 141\n",
      "epoch: 142\n",
      "epoch: 143\n",
      "epoch: 144\n",
      "epoch: 145\n",
      "epoch: 146\n",
      "epoch: 147\n",
      "epoch: 148\n",
      "epoch: 149\n",
      "epoch: 150\n",
      "epoch: 151\n",
      "epoch: 152\n",
      "epoch: 153\n",
      "epoch: 154\n",
      "epoch: 155\n",
      "epoch: 156\n",
      "epoch: 157\n",
      "epoch: 158\n",
      "epoch: 159\n",
      "epoch: 160\n",
      "epoch: 161\n",
      "epoch: 162\n",
      "epoch: 163\n",
      "epoch: 164\n",
      "epoch: 165\n",
      "epoch: 166\n",
      "epoch: 167\n",
      "epoch: 168\n",
      "epoch: 169\n",
      "epoch: 170\n",
      "epoch: 171\n",
      "epoch: 172\n",
      "epoch: 173\n",
      "epoch: 174\n",
      "epoch: 175\n",
      "epoch: 176\n",
      "epoch: 177\n",
      "epoch: 178\n",
      "epoch: 179\n",
      "epoch: 180\n",
      "epoch: 181\n",
      "epoch: 182\n",
      "epoch: 183\n",
      "epoch: 184\n",
      "epoch: 185\n",
      "epoch: 186\n",
      "epoch: 187\n",
      "epoch: 188\n",
      "epoch: 189\n",
      "epoch: 190\n",
      "epoch: 191\n",
      "epoch: 192\n",
      "epoch: 193\n",
      "epoch: 194\n",
      "epoch: 195\n",
      "epoch: 196\n",
      "epoch: 197\n",
      "epoch: 198\n",
      "epoch: 199\n",
      "epoch: 200\n",
      "epoch: 201\n",
      "epoch: 202\n",
      "epoch: 203\n",
      "epoch: 204\n",
      "epoch: 205\n",
      "epoch: 206\n",
      "epoch: 207\n",
      "epoch: 208\n",
      "epoch: 209\n",
      "epoch: 210\n",
      "epoch: 211\n",
      "epoch: 212\n",
      "epoch: 213\n",
      "epoch: 214\n",
      "epoch: 215\n",
      "epoch: 216\n",
      "epoch: 217\n",
      "epoch: 218\n",
      "epoch: 219\n",
      "epoch: 220\n",
      "epoch: 221\n",
      "epoch: 222\n",
      "epoch: 223\n",
      "epoch: 224\n",
      "epoch: 225\n",
      "epoch: 226\n",
      "epoch: 227\n",
      "epoch: 228\n",
      "epoch: 229\n",
      "epoch: 230\n",
      "epoch: 231\n",
      "epoch: 232\n",
      "epoch: 233\n",
      "epoch: 234\n",
      "epoch: 235\n",
      "epoch: 236\n",
      "epoch: 237\n",
      "epoch: 238\n",
      "epoch: 239\n",
      "epoch: 240\n",
      "epoch: 241\n",
      "epoch: 242\n",
      "epoch: 243\n",
      "epoch: 244\n",
      "epoch: 245\n",
      "epoch: 246\n",
      "epoch: 247\n",
      "epoch: 248\n",
      "epoch: 249\n",
      "epoch: 250\n",
      "epoch: 251\n",
      "epoch: 252\n",
      "epoch: 253\n",
      "epoch: 254\n",
      "epoch: 255\n",
      "epoch: 256\n",
      "epoch: 257\n",
      "epoch: 258\n",
      "epoch: 259\n",
      "epoch: 260\n",
      "epoch: 261\n",
      "epoch: 262\n",
      "epoch: 263\n",
      "epoch: 264\n",
      "epoch: 265\n",
      "epoch: 266\n",
      "epoch: 267\n",
      "epoch: 268\n",
      "epoch: 269\n",
      "epoch: 270\n",
      "epoch: 271\n",
      "epoch: 272\n",
      "epoch: 273\n",
      "epoch: 274\n",
      "epoch: 275\n",
      "epoch: 276\n",
      "epoch: 277\n",
      "epoch: 278\n",
      "epoch: 279\n",
      "epoch: 280\n",
      "epoch: 281\n",
      "epoch: 282\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-198-ee64c75b27b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_sample\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m#weights update (learning)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-122-dfe74f5f2102>\u001b[0m in \u001b[0;36mget_gradient\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_len\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;31m#hidden_layer의 수 + output_layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"W\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_numerical_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_W\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"W\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m             \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"b\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_numerical_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_W\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"b\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-122-dfe74f5f2102>\u001b[0m in \u001b[0;36mweight_numerical_gradient\u001b[0;34m(self, f, x)\u001b[0m\n\u001b[1;32m     58\u001b[0m                 \u001b[0mfxh1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemp\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m                 \u001b[0mfxh2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0mgrad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfxh1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mfxh2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-122-dfe74f5f2102>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(W)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;31m#수치미분 함수를 이용해 각 가중치에 대응하는 gradient 사전을 얻음\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mloss_W\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-122-dfe74f5f2102>\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;31m#x: 입력 데이터, t: 정답 레이블\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mCEE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-122-dfe74f5f2102>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_len\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"W\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"b\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mi\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"W\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"b\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-b6a5868b4fd5>\u001b[0m in \u001b[0;36msigmoid_function\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#sigmoid_function : 주어진 값을 0~1 사이의 값으로 변환해주는 S자 모양의 함수. 0보다 크면 0.5보다 크게, 0보다 작으면 0.5보다 작게 변환된다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msigmoid_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(0, iters_num) : \n",
    "    print(\"epoch:\",i)\n",
    "    '''\n",
    "    #순차적으로 mini-batch\n",
    "    if (i+1)*batch_size > train_size : \n",
    "        x_batch = x_train[train_size-i*batch_size:]\n",
    "        y_batch = y_train[train_size-i*batch_size:]\n",
    "    else : \n",
    "        x_batch = x_train[i*batch_size:(i+1)*batch_size]\n",
    "        y_batch = y_train[i*batch_size:(i+1)*batch_size]\n",
    "    '''\n",
    "    \n",
    "    #복원 추출로 랜덤 mini-batch\n",
    "    batch_sample = np.random.choice(train_size, batch_size) \n",
    "    x_batch = x_train[batch_sample]\n",
    "    y_batch = y_train[batch_sample]\n",
    "    \n",
    "    grad = net.get_gradient(x_batch, y_batch)\n",
    "    \n",
    "    #weights update (learning)\n",
    "    for key in net.params.keys() : \n",
    "        net.params[key] -= learning_rate*grad[key]\n",
    "        \n",
    "    #learning result check\n",
    "    loss = net.loss(x_batch, y_batch) \n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    if i%iter_per_epoch == 0 : \n",
    "        train_acc = net.accuracy(x_train, y_train)\n",
    "        test_acc = net.accuracy(x_test, y_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"epoch: %5d]]] loss: %f\\ttrai acc: %f, test acc: %f\"%(i, loss, train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
